{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This cell is responsible for managing imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from qiskit import Aer          # for simulator backend\n",
    "from qiskit_machine_learning.algorithms import QSVC # quantum support vector classifier class\n",
    "from qiskit_machine_learning.kernels.quantum_kernel import QuantumKernel # wraps feature map and backend combination to give to QSVC\n",
    "import qiskit.circuit.library   # for feature maps\n",
    "import numpy as np\n",
    "import joblib                   # for persistence\n",
    "\n",
    "# for data sets and data set processing\n",
    "import sklearn.datasets\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from qiskit_machine_learning.datasets.dataset_helper import features_and_labels_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The functions in this cell are responsible for loading, preparing, and splitting data sets for input to the QSVM classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: change this to allow specifying which classes to extract for binary classification,\n",
    "# rather than simply extracting 2 arbitrary classes. If 2 arbitrary classes are specified anyway,\n",
    "# at least make them manually selected and identifiable rather than seemingly random.\n",
    "def extract_binary_classes(feature_array, label_array):\n",
    "    \"\"\"Takes a numpy array of feature vectors and a numpy array of labels\n",
    "    and returns transformed numpy arrays with the number of classes reduced\n",
    "    to 2.\"\"\"\n",
    "    classes = list(set(label_array))[:2] # get the first 2 unique labels as classes\n",
    "    class_map = {classes[0]:0, classes[1]:1} # convert labels to 0 and 1 (needed for training step)\n",
    "    # construct a feature and label description with information from only the first 2 classes\n",
    "    features = []\n",
    "    labels = []\n",
    "    for (feature, label) in zip(feature_array, label_array):\n",
    "        if label in classes:\n",
    "            features.append(feature)\n",
    "            labels.append(label)\n",
    "    return (np.array(features), np.array(labels))\n",
    "\n",
    "def process_dataset(dataset, qubit_count=4, binary_classification=True):\n",
    "    \"\"\"Performs scaling and dimensionality reduction on all feature vectors of a data set,\n",
    "    then returns the processed vectors. It will also extract 2 classes from the data set\n",
    "    if binary_classification is True, rather than leaving the data set as a multi-class data set.\n",
    "    Some of this code is modified from the qiskit_machine_learning data set loading\n",
    "    source code (check qiskit_machine_learning.datasets.digits source code for exact location\n",
    "    of what was modified from).\"\"\"\n",
    "    feature_vectors = dataset.data\n",
    "    labels = dataset.target\n",
    "\n",
    "    # maybe extract classes for binary classification\n",
    "    if binary_classification:\n",
    "        feature_vectors, labels = extract_binary_classes(feature_vectors, labels)\n",
    "\n",
    "    # Now we standardize for gaussian around 0 with unit variance\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(feature_vectors)\n",
    "    feature_vectors = scaler.transform(feature_vectors)\n",
    "\n",
    "    # Now reduce number of features to number of qubits\n",
    "    pca = PCA(n_components=qubit_count)\n",
    "    pca.fit(feature_vectors)\n",
    "    feature_vectors = pca.transform(feature_vectors)\n",
    "\n",
    "    # Scale to the range (-1,+1)\n",
    "    minmax_scaler = MinMaxScaler((-1, 1)).fit(feature_vectors)\n",
    "    feature_vectors = minmax_scaler.transform(feature_vectors)\n",
    "\n",
    "    # perform some other transformation on the feature and label vectors\n",
    "    # as was done in the qiskit_machine_learning source code\n",
    "    dataset_dict = {label:np.array([feature_vector for feature_vector, feature_vector_label in zip(feature_vectors, labels)\n",
    "                                    if feature_vector_label == label])\n",
    "                    for label in list(set(labels))}\n",
    "    feature_vectors, labels = features_and_labels_transform(dataset_dict, labels, one_hot=False)\n",
    "\n",
    "    return feature_vectors, labels\n",
    "\n",
    "def cross_fold_sets(data, labels, k=5, seed=22):\n",
    "    \"Given a data set's feature array, yield training and testing feature arrays for k-fold validation. If the same seed is used then the same subsets should be returned across different calls.\"\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "    # for each of the k train-test splits:\n",
    "    for train_indices, test_indices in kf.split(data, labels):\n",
    "        # helper function\n",
    "        extract_elements = lambda array, indices: np.array([array[i] for i in indices])\n",
    "        # get training and testing feature vectors\n",
    "        train_features = extract_elements(data, train_indices)\n",
    "        test_features = extract_elements(data, test_indices)\n",
    "        # get training and testing labels\n",
    "        train_labels = extract_elements(labels, train_indices)\n",
    "        test_labels = extract_elements(labels, test_indices)\n",
    "        # return current split values\n",
    "        yield (train_features, train_labels, test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This cell defines a function that can be given some parameters determining a classifier, like the feature map to use, the data to train on, and the backend to run the training on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MAYBE DO: make batch size a parameter\n",
    "def make_classifier(feature_map_instance, training_features, training_labels, backend):\n",
    "    \"\"\"Given a feature map instance, training features and labels, and a quantum backend,\n",
    "    creates, trains, and returns a QSVM classifier.\"\"\"\n",
    "    # Create a quantum kernel from the feature map and\n",
    "    # backend to give to the QSVC class.\n",
    "    batch_size = 1000           # this is the QuantumKernel default\n",
    "    quantum_kernel = QuantumKernel(feature_map=feature_map_instance, batch_size=batch_size, quantum_instance=backend)\n",
    "    # Create a QSVC instance\n",
    "    qsvc = QSVC(quantum_kernel=quantum_kernel)\n",
    "    # Perform training\n",
    "    qsvc.fit(training_features, training_labels)\n",
    "    # return classifier instance\n",
    "    return qsvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This cell is similar to the above cell in that it in effect takes a specification for a classifier, but the function instead returns the generalisation metrics of the classifier that is described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: finalize what generalisation metrics should be used and calculate them (maybe also look at margin size)\n",
    "# MAYBE DO: put parameters like feature count and  number of repetitions in\n",
    "# the argument list to make them independent variables of the experiments rather\n",
    "# than constants.\n",
    "def process_combination(feature_map_class, data_split_tuple, repetitions, backend_instance, qubit_count=4):\n",
    "    \"\"\"Takes a feature map class, dataset loading function, and a backend, and\n",
    "    returns the generalisation metrics of the combination of arguments.\"\"\"\n",
    "    # Create the feature map instance.\n",
    "    feature_count = qubit_count\n",
    "    feature_map_instance = feature_map_class(feature_dimension=feature_count, reps=repetitions)\n",
    "    # unpack the data split for binary classification\n",
    "    train_features, train_labels, test_features, test_labels = data_split_tuple\n",
    "\n",
    "    # create the classifier\n",
    "    qsvc = make_classifier(feature_map_instance, train_features, train_labels, backend_instance)\n",
    "\n",
    "    # get the classification accuracy on training and testing data as generalisation metrics\n",
    "    train_accuracy = qsvc.score(train_features, train_labels)\n",
    "    test_accuracy = qsvc.score(test_features, test_labels)\n",
    "    # return the generalisation metrics and the trained model\n",
    "    return train_accuracy, test_accuracy, qsvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This cell defines a function that collects the generalisation information of all tested classifier configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: make k-fold validation gather statistics of results in this\n",
    "# function or later, rather than treating each of the k runs as a separate\n",
    "# combination.\n",
    "combination_number = 0 # must be global to work around python scope limitations for nested functions\n",
    "def process_all_combinations(do_binary_classification=True):\n",
    "    \"\"\"Runs all experiments.\"\"\"\n",
    "    global combination_number\n",
    "    # get a list of datasets loaded into memory\n",
    "    datasets = [sklearn.datasets.load_breast_cancer(),\n",
    "                sklearn.datasets.load_digits(),\n",
    "                sklearn.datasets.load_iris(),\n",
    "                sklearn.datasets.load_wine()]\n",
    "    # corresponding human-readable names for recording results\n",
    "    dataset_names = [\"cancer\", \"digits\", \"iris\", \"wine\"]\n",
    "\n",
    "    # get a list of feature maps\n",
    "    feature_map_classes = [qiskit.circuit.library.PauliFeatureMap,\n",
    "                           qiskit.circuit.library.ZFeatureMap,\n",
    "                           qiskit.circuit.library.ZZFeatureMap]\n",
    "    # corresponding human-readable names for recording results\n",
    "    feature_map_names = [\"Pauli\", \"Z\", \"ZZ\"]\n",
    "\n",
    "    # Number of qubits to simulate / number of features to reduce to.\n",
    "    qubit_count = 4\n",
    "\n",
    "    # create a quantum backend\n",
    "    backend = Aer.get_backend(\"aer_simulator_statevector\") # should configure this to mimic IBMQ backend\n",
    "    #backend.set_options(device='GPU')                      # enable GPU acceleration (comment this line to disable)\n",
    "\n",
    "    # Define choice of k for k-fold cross-validation. This\n",
    "    # number determines how many equally sized disjoint\n",
    "    # subsets to split the dataset into, after which each\n",
    "    # is used as the testing set in turn with the remaining\n",
    "    # subsets being used as the training set.\n",
    "    cross_validation_splits = 5\n",
    "    \n",
    "    # Do some output to the user to give them a sense of how long running the experiments will take\n",
    "    number_of_investigated_repetition_values = 4 # for trying depth = 2, 3, 4, and 5\n",
    "    combination_count = len(datasets) * cross_validation_splits * (number_of_investigated_repetition_values + len(feature_map_classes)-1)\n",
    "    print(f\"Running with {len(datasets)} datasets, {len(feature_map_classes)} feature maps, {number_of_investigated_repetition_values} different encoding repetitions for the ZZ feature map, and {cross_validation_splits}-fold cross validation, requiring the training of {combination_count} classifiers in total.\")\n",
    "    \n",
    "    ## Process each combination.\n",
    "    results = {}\n",
    "    combination_number = 0      # set global combination number to 0\n",
    "    # For each dataset\n",
    "    for dataset, dataset_name in zip(datasets, dataset_names):\n",
    "        # Perform dimensionality reduction and scaling on the features,\n",
    "        # and transform the labels as done in the qiskit_machine_learning\n",
    "        # data set loading source code. Also prepares the data for\n",
    "        # binary rather than multi-class classification if enabled.\n",
    "        features, labels = process_dataset(dataset, binary_classification=do_binary_classification, qubit_count=qubit_count)\n",
    "        print(f\"{dataset_name} dataset: {len(features)} feature vectors.\")\n",
    "        # For each k-fold split of the data into training and testing sets\n",
    "        for (split_number, split_tuple) in enumerate(cross_fold_sets(features, labels, k=cross_validation_splits)):\n",
    "            # For each feature map\n",
    "            for feature_map_class, feature_map_name in zip(feature_map_classes, feature_map_names):\n",
    "                def process_with_repetitions(repetitions):\n",
    "                    global combination_number # use reference to global variable\n",
    "                    # show a debug / progress message\n",
    "                    print(f\"{combination_number}: Processing data set {dataset_name}, feature map {feature_map_name}, cross-validation split {split_number}, and {repetitions} encoding repetitions...\")\n",
    "                    combination_number += 1 # for debug / progress output\n",
    "                    # run experiment for this combination\n",
    "                    train_accuracy, test_accuracy, model = process_combination(feature_map_class, split_tuple, repetitions, backend, qubit_count)\n",
    "                    # record results using human-readable names and values (other than the trained classifier)\n",
    "                    results[(dataset_name, split_number, feature_map_name, repetitions)] = (train_accuracy, test_accuracy)\n",
    "                # For ZZ feature map, try multiple encoding repetitions. For other feature maps,\n",
    "                # just do 2 repetitions.\n",
    "                if feature_map_name == \"ZZ\":\n",
    "                    for reps in range(2, 2+number_of_investigated_repetition_values):\n",
    "                        process_with_repetitions(reps)\n",
    "                else:\n",
    "                    process_with_repetitions(2)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This cell defines a function that performs the experiments and reports their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main ():\n",
    "    results = process_all_combinations()\n",
    "    for combination in results:\n",
    "        print(f\"Combination {combination} has results {results[combination]}.\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This cell can be evaluated to actually perform the experiments. It can take a few hours to run so loading pre-computed results is preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#results = main()                # uncomment this when you want to run it (to prevent running accidentally)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The next 3 cells should be run selectively to save and load pre-computed results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_results():\n",
    "    \"\"\"Saves results to a file if there are any, overwriting previous results.\"\"\"\n",
    "    if results != None:\n",
    "        joblib.dump(results, \"results.z\")\n",
    "def load_results():\n",
    "    \"\"\"Loads results from a file.\"\"\"\n",
    "    global results\n",
    "    results = joblib.load(\"results.z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save_results() # uncomment this when you want to run it (to prevent accidentally running it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load_results() # uncomment this when you want to run it (to prevent accidentally running it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "These 2 cells read the results variable and combine cross-validation runs to get statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def combine_cross_validations():\n",
    "    def extract_cross_validations(combination):\n",
    "        \"\"\"Returns a list of training and a list of testing accuracies for the given\n",
    "        combination, using the values in the results from the different cross-validation\n",
    "        runs.\"\"\"\n",
    "        dataset_name, feature_map_name, repetitions = combination\n",
    "        train_list = []\n",
    "        test_list = []\n",
    "        for key in results:\n",
    "            (d_name, run, f_name, rep) = key\n",
    "            if d_name == dataset_name and f_name == feature_map_name and rep == repetitions:\n",
    "                train_accuracy, test_accuracy = results[key]\n",
    "                train_list.append(train_accuracy)\n",
    "                test_list.append(test_accuracy)\n",
    "        return train_list, test_list\n",
    "    # extract combinations to get statistics about from the results variable\n",
    "    combinations = set()\n",
    "    for key in results:\n",
    "        (d_name, run, f_name, rep) = key\n",
    "        combinations.add((d_name, f_name, rep))\n",
    "    stats = {}\n",
    "    for c in combinations:\n",
    "        train_accuracies, test_accuracies = extract_cross_validations(c)\n",
    "        stats[c] = ((np.mean(train_accuracies), np.std(train_accuracies), np.var(train_accuracies)),\n",
    "                    (np.mean(test_accuracies), np.std(test_accuracies), np.var(test_accuracies)))\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This should only run after computing or loading a value for the results variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for wine, Pauli, 2 repetitions:\n",
      "Mean classifier accuracies:\n",
      "0.9673076923076923 | 0.7461538461538462\n",
      "Standard deviations:\n",
      "0.014390989949130531 | 0.06249260311258431\n",
      "Variances:\n",
      "0.00020710059171597596 | 0.003905325443786982\n",
      "\n",
      "Stats for cancer, Pauli, 2 repetitions:\n",
      "Mean classifier accuracies:\n",
      "0.9507923655292077 | 0.8541996584381307\n",
      "Standard deviations:\n",
      "0.008737959335838347 | 0.03808109446036603\n",
      "Variances:\n",
      "7.635193335476453e-05 | 0.0014501697552993203\n",
      "\n",
      "Stats for iris, Pauli, 2 repetitions:\n",
      "Mean classifier accuracies:\n",
      "0.9875 | 0.7\n",
      "Standard deviations:\n",
      "0.007905694150420955 | 0.0894427190999916\n",
      "Variances:\n",
      "6.250000000000011e-05 | 0.008000000000000004\n",
      "\n",
      "Stats for digits, Pauli, 2 repetitions:\n",
      "Mean classifier accuracies:\n",
      "0.992361111111111 | 0.9583333333333334\n",
      "Standard deviations:\n",
      "0.004049272149198111 | 0.008784104611578835\n",
      "Variances:\n",
      "1.639660493827149e-05 | 7.716049382716056e-05\n",
      "\n",
      "Stats for wine, Z, 2 repetitions:\n",
      "Mean classifier accuracies:\n",
      "0.9442307692307693 | 0.9230769230769231\n",
      "Standard deviations:\n",
      "0.01538461538461538 | 0.042132504423474326\n",
      "Variances:\n",
      "0.00023668639053254424 | 0.0017751479289940838\n",
      "\n",
      "Stats for cancer, Z, 2 repetitions:\n",
      "Mean classifier accuracies:\n",
      "0.9543069211490265 | 0.9490296537804689\n",
      "Standard deviations:\n",
      "0.007134302409718124 | 0.022475147657851927\n",
      "Variances:\n",
      "5.089827087330983e-05 | 0.000505132262242247\n",
      "\n",
      "Stats for iris, Z, 2 repetitions:\n",
      "Mean classifier accuracies:\n",
      "0.8800000000000001 | 0.77\n",
      "Standard deviations:\n",
      "0.018708286933869722 | 0.0748331477354788\n",
      "Variances:\n",
      "0.0003500000000000006 | 0.005599999999999997\n",
      "\n",
      "Stats for digits, Z, 2 repetitions:\n",
      "Mean classifier accuracies:\n",
      "0.98125 | 0.9805555555555557\n",
      "Standard deviations:\n",
      "0.003540985773328341 | 0.01416394309331329\n",
      "Variances:\n",
      "1.2538580246913708e-05 | 0.00020061728395061724\n",
      "\n",
      "Stats for wine, ZZ, 2 repetitions:\n",
      "Mean classifier accuracies:\n",
      "0.9673076923076923 | 0.7461538461538462\n",
      "Standard deviations:\n",
      "0.014390989949130531 | 0.06249260311258431\n",
      "Variances:\n",
      "0.00020710059171597596 | 0.003905325443786982\n",
      "\n",
      "Stats for cancer, ZZ, 2 repetitions:\n",
      "Mean classifier accuracies:\n",
      "0.9507923655292077 | 0.8541996584381307\n",
      "Standard deviations:\n",
      "0.008737959335838347 | 0.03808109446036603\n",
      "Variances:\n",
      "7.635193335476453e-05 | 0.0014501697552993203\n",
      "\n",
      "Stats for iris, ZZ, 2 repetitions:\n",
      "Mean classifier accuracies:\n",
      "0.9875 | 0.7\n",
      "Standard deviations:\n",
      "0.007905694150420955 | 0.0894427190999916\n",
      "Variances:\n",
      "6.250000000000011e-05 | 0.008000000000000004\n",
      "\n",
      "Stats for digits, ZZ, 2 repetitions:\n",
      "Mean classifier accuracies:\n",
      "0.992361111111111 | 0.9583333333333334\n",
      "Standard deviations:\n",
      "0.004049272149198111 | 0.008784104611578835\n",
      "Variances:\n",
      "1.639660493827149e-05 | 7.716049382716056e-05\n",
      "\n",
      "Stats for wine, ZZ, 3 repetitions:\n",
      "Mean classifier accuracies:\n",
      "0.9538461538461538 | 0.5846153846153846\n",
      "Standard deviations:\n",
      "0.018644922528524326 | 0.08213137116947161\n",
      "Variances:\n",
      "0.000347633136094674 | 0.006745562130177514\n",
      "\n",
      "Stats for cancer, ZZ, 3 repetitions:\n",
      "Mean classifier accuracies:\n",
      "0.8923558897243108 | 0.7645551932929668\n",
      "Standard deviations:\n",
      "0.006200008967158313 | 0.02754057420141026\n",
      "Variances:\n",
      "3.84401111928435e-05 | 0.0007584832273433845\n",
      "\n",
      "Stats for iris, ZZ, 3 repetitions:\n",
      "Mean classifier accuracies:\n",
      "0.9799999999999999 | 0.48\n",
      "Standard deviations:\n",
      "0.0061237243569579785 | 0.06782329983125267\n",
      "Variances:\n",
      "3.75000000000004e-05 | 0.004599999999999998\n",
      "\n",
      "Stats for digits, ZZ, 3 repetitions:\n",
      "Mean classifier accuracies:\n",
      "0.9458333333333332 | 0.7777777777777777\n",
      "Standard deviations:\n",
      "0.008098544298396272 | 0.04648111258522644\n",
      "Variances:\n",
      "6.558641975308676e-05 | 0.002160493827160496\n",
      "\n",
      "Stats for wine, ZZ, 4 repetitions:\n",
      "Mean classifier accuracies:\n",
      "0.9519230769230769 | 0.6538461538461539\n",
      "Standard deviations:\n",
      "0.010533126105868582 | 0.03440104580768905\n",
      "Variances:\n",
      "0.00011094674556213024 | 0.0011834319526627204\n",
      "\n",
      "Stats for cancer, ZZ, 4 repetitions:\n",
      "Mean classifier accuracies:\n",
      "0.8536948139579719 | 0.7188169538891476\n",
      "Standard deviations:\n",
      "0.009419792474876211 | 0.04828730825978787\n",
      "Variances:\n",
      "8.873249026973448e-05 | 0.002331664138975778\n",
      "\n",
      "Stats for iris, ZZ, 4 repetitions:\n",
      "Mean classifier accuracies:\n",
      "0.9650000000000001 | 0.55\n",
      "Standard deviations:\n",
      "0.020000000000000025 | 0.04472135954999579\n",
      "Variances:\n",
      "0.00040000000000000105 | 0.001999999999999999\n",
      "\n",
      "Stats for digits, ZZ, 4 repetitions:\n",
      "Mean classifier accuracies:\n",
      "0.9423611111111111 | 0.7805555555555556\n",
      "Standard deviations:\n",
      "0.010668257983150839 | 0.02693155476342406\n",
      "Variances:\n",
      "0.00011381172839506162 | 0.0007253086419753092\n",
      "\n",
      "Stats for wine, ZZ, 5 repetitions:\n",
      "Mean classifier accuracies:\n",
      "0.9519230769230769 | 0.5692307692307692\n",
      "Standard deviations:\n",
      "0.01359820733051053 | 0.15268794800984006\n",
      "Variances:\n",
      "0.0001849112426035503 | 0.023313609467455622\n",
      "\n",
      "Stats for cancer, ZZ, 5 repetitions:\n",
      "Mean classifier accuracies:\n",
      "0.8431434355118566 | 0.6590436267660301\n",
      "Standard deviations:\n",
      "0.009526636360837813 | 0.04058538764318259\n",
      "Variances:\n",
      "9.075680035163711e-05 | 0.0016471736901473983\n",
      "\n",
      "Stats for iris, ZZ, 5 repetitions:\n",
      "Mean classifier accuracies:\n",
      "0.9625 | 0.42000000000000004\n",
      "Standard deviations:\n",
      "0.030618621784789746 | 0.07483314773547885\n",
      "Variances:\n",
      "0.0009375000000000012 | 0.005600000000000003\n",
      "\n",
      "Stats for digits, ZZ, 5 repetitions:\n",
      "Mean classifier accuracies:\n",
      "0.9256944444444445 | 0.7555555555555555\n",
      "Standard deviations:\n",
      "0.006440012844094266 | 0.04082482904638632\n",
      "Variances:\n",
      "4.147376543209912e-05 | 0.0016666666666666683\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stats = combine_cross_validations()\n",
    "# TODO: this below loop should be made more robust if it stays in the code\n",
    "for feature_map in (\"Pauli\", \"Z\", \"ZZ\"):\n",
    "    for repetition in range(2, 6 if feature_map == \"ZZ\" else 3):\n",
    "        for dataset in (\"wine\", \"cancer\", \"iris\", \"digits\"):\n",
    "            key = (dataset, feature_map, repetition)\n",
    "            value = stats[key]\n",
    "            print(f\"Stats for {dataset}, {feature_map}, {repetition} repetitions:\")\n",
    "            print(\"Mean classifier accuracies:\")\n",
    "            print(f\"{value[0][0]} | {value[1][0]}\")\n",
    "            print(\"Standard deviations:\")\n",
    "            print(f\"{value[0][1]} | {value[1][1]}\")\n",
    "            print(\"Variances:\")\n",
    "            print(f\"{value[0][2]} | {value[1][2]}\")\n",
    "            print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "C:/ProgramData/Anaconda3\\python.exe",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "name": "project.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
